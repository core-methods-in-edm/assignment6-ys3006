---
title: "Assignment 6_code"
author: "ys3006"
date: "November 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## pck + data
```{r}
library(rpart)
M1 <- read.csv("MOOC1.csv",header = TRUE)
M2 <- read.csv("MOOC2.csv",header = TRUE)
```

## Decision Tree
```{r}
c.tree1 <- rpart(certified ~ forum.posts + grade + assignment, method = "class", data = M1, control = rpart.control(minsplit = 1,minbucket = 1,cp=0.001))

# look at the error of this tree
printcp(c.tree1)



# Plot the tree with a pdf image of tree
post(c.tree1, file = "tree1.ps", title="MOOC")
```


## The heading "xerror" in the printcp table stands for "cross validation error", it is the error rate of assigning students to certified/uncertified of the model averaged over 10-fold cross validation. CP stands for "Cost Complexity" and represents the cost in error for adding a node to the tree. Notice it decreases as we add more nodes to the tree which implies that more nodes make better predictions. However, more nodes also mean that we may be making the model less generalizable, this is known as "overfitting".

## If we are worried about overfitting we can remove nodes form our tree using the prune() command, setting cp to the CP value from the table that corresponds to the number of nodes we want the tree to terminate at. Let's set it to two nodes.
```{r}
# since the second node in tree 1 has CP 0.014545, for two nodes, set cp to the value larger than 0.014545 as close as possible
c.tree2 <- prune(c.tree1, cp = 0.015)
printcp(c.tree2)
# visualize
post(c.tree2, file="tree2.ps",title="MOOC")
```

# prediction
```{r}
M2$predict1 <- predict(c.tree1, M2, type = "class")
M2$predict2 <- predict(c.tree2, M2, type = "class")
T1 = table(M2$certified, M2$predict1)
T2 = table(M2$certified, M2$predict2)

# calculate the error rate for tree 1
(T1[1] + T1[4])/sum(T1)
# error rate for tree 2
(T2[1] + T2[4])/sum(T2)

# the second tree model has lower error rate at 52.73%, whereas first tree model has 21.71%
```